# -*- coding: utf-8 -*-
"""Untitled16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lH2MsJ5tVVJeijig-L1jo2Ojk1wR-7kL

You are provided with a dataset from USA Forensic Science Service which has description of 6 types of glass; defined in terms of their oxide content (i.e. Na, Fe, K, etc). Your task is to use K-Nearest Neighbor (KNN) classifier to classify the glasses.

The original dataset is available at

(https://archive.ics.uci.edu/ml/datasets/glass+identification). For detailed description on the attributes of the dataset,

please refer to the original link of the dataset in the UCI ML repository.

But the shared drive folder have the dataset for your convenience

perform exploratory data analysis on the dataset using Python Pandas, including dropping irrelevant fields for predicted values, and standardization of each attribute.

Following data cleaning, two Scikit-Learn KNN models should be created for two different distance metrics: Square Euclidean and Manhattan distance. The performance of the two models using different distance metrics should be compared in terms of accuracy to the test data and Scikit-Learn Classification Report.

Attribute Information:
Id number: 1 to 214
RI: refractive index
Na: Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)
Mg: Magnesium
Al: Aluminum
Si: Silicon
K: Potassium
Ca: Calcium
Ba: Barium
Fe: Iron
Type of glass: (class attribute) -- 1 building_windows_float_processed -- 2 building_windows_non_float_processed -- 3 vehicle_windows_float_processed -- 4 vehicle_windows_non_float_processed (none in this database) -- 5 containers -- 6 tableware -- 7 headlamps

1.IMPORT LIBRARIES
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.neighbors import KNeighborsClassifier 
from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report

train = pd.read_csv("trainKNN.txt" , header=None,
                 names=["Id number", "RI", "Na", "Mg", "Al", "Si","K","Ca","Ba",'Fe','Type of glass'],index_col='Id number')

test = pd.read_csv("testKNN.txt" , header=None,
                 names=["Id number", "RI", "Na", "Mg", "Al", "Si","K","Ca","Ba",'Fe','Type of glass'],index_col='Id number')

train.head(5)

test.head(5)

print(train.shape)
print(test.shape)

print(train.info())

print(test.info())

train.isnull().sum()

test.isnull().sum()

x=train.drop(columns='Type of glass')

for col in x:
    skew = train[col].skew()
    sns.distplot(train[col],kde=False,label='Skew = %.2f' %(skew),bins=100)
    plt.legend(loc='best')
    plt.show()

"""***TRAIN***"""

x_train = train.drop(columns=['Type of glass',],axis=1)

x_test = test.drop(columns=['Type of glass'],axis=1)

y_train = train['Type of glass']

y_test = test['Type of glass']

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
x_train = scaler.fit_transform(x_train)
x_test = scaler.transform(x_test)

"""## ***EUCLIDEAN METRIC***"""

classifier= KNeighborsClassifier(n_neighbors=5,metric='euclidean', p=2 )  
classifier.fit(x_train, y_train)  
KNeighborsClassifier(metric='euclidean')
euclid_pred = classifier.predict(x_test)
euclid_score = accuracy_score(y_test,euclid_pred)
euclid_score = euclid_score*100

print("Accuracy Score of Euclidean Metric : \t ",euclid_score)

classifier.score(x_test,y_test)*100

"""**MANHATTEN** ***MATRIX***"""

classifier= KNeighborsClassifier(metric='manhattan')  
classifier.fit(x_train, y_train)

manhat_pred = classifier.predict(x_test)
manhat_score= accuracy_score(y_test,manhat_pred)*100
print("Accuracy score of Manhattan metric : \t ",manhat_score)

print(classification_report(y_test,manhat_pred))